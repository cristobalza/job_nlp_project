{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#NLP\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import string, re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV \n",
    "\n",
    "#Model Data Prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Deep learning\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier #to use Keras in sklearn\n",
    "\n",
    "#Deep Learning - Mixed inputs\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "#Save Model\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/final_only_salary_us_data_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1520 entries, 0 to 1519\n",
      "Data columns (total 25 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   Unnamed: 0                      1520 non-null   int64  \n",
      " 1   job_title                       1520 non-null   object \n",
      " 2   salary                          1520 non-null   object \n",
      " 3   company                         1520 non-null   object \n",
      " 4   location                        1520 non-null   object \n",
      " 5   is_remote                       530 non-null    object \n",
      " 6   job_rating                      857 non-null    float64\n",
      " 7   job_summary                     1520 non-null   object \n",
      " 8   post_date                       1520 non-null   object \n",
      " 9   extract_date                    1520 non-null   object \n",
      " 10  job_url                         1520 non-null   object \n",
      " 11  rate_by                         1520 non-null   object \n",
      " 12  min                             1520 non-null   float64\n",
      " 13  max                             1520 non-null   float64\n",
      " 14  adjusted_salary                 1520 non-null   float64\n",
      " 15  yearly_adjusted_salary          1520 non-null   float64\n",
      " 16  salary_bins                     1520 non-null   object \n",
      " 17  Cost of Living Index            906 non-null    float64\n",
      " 18  Rent Index                      906 non-null    float64\n",
      " 19  Cost of Living Plus Rent Index  906 non-null    float64\n",
      " 20  Groceries Index                 906 non-null    float64\n",
      " 21  Restaurant Price Index          906 non-null    float64\n",
      " 22  Local Purchasing Power Index    906 non-null    float64\n",
      " 23  city_bins                       906 non-null    object \n",
      " 24  combined_text                   1520 non-null   object \n",
      "dtypes: float64(11), int64(1), object(13)\n",
      "memory usage: 297.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_bins = [0,50000,70000,90000,120000,150000, 300000, 600000]\n",
    "df['salary_bins'] = pd.cut(df['yearly_adjusted_salary'],\n",
    "                                         bins=cut_bins, \n",
    "                                         labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       5\n",
       "2       3\n",
       "3       4\n",
       "4       2\n",
       "       ..\n",
       "1515    3\n",
       "1516    0\n",
       "1517    0\n",
       "1518    1\n",
       "1519    2\n",
       "Name: salary_bins, Length: 1520, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['salary_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(job_descriptions):\n",
    "    jd_data=[]\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += list(string.punctuation)\n",
    "    stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    \n",
    "    for jd in job_descriptions:\n",
    "        jd_tokens_raw = nltk.regexp_tokenize(jd, pattern)\n",
    "        jd_tokens=[word.lower() for word in jd_tokens_raw]\n",
    "        jd_words_stopped = [word for word in jd_tokens if word not in stopwords_list]\n",
    "        jd_data.append(jd_words_stopped)\n",
    "    return pd.Series(jd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['combined_text']\n",
    "target_class = df['salary_bins'].astype('category')\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, target_class, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This works for individual job description, but how to save the model? \n",
    "tdidf = TfidfVectorizer(sublinear_tf = True, min_df=0.01, max_df=0.5, ngram_range=(1,3), stop_words='english')\n",
    "fitted_vectorizer = tdidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "model = SGDClassifier(alpha=0.0001, max_iter=500, n_jobs=3).fit(tfidf_vectorizer_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the vectorizer to disk\n",
    "\n",
    "pickle.dump(fitted_vectorizer, open(\"./models/fitted_vectorizer.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the machine learning model to disk\n",
    "\n",
    "filename='finalized_model_2.h5'\n",
    "pickle.dump(model, open('./models/'+filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the machine model using joblib\n",
    "\n",
    "from joblib import dump, load\n",
    "dump(model, './models/finalized_model_2.joblib')\n",
    "clf = load('./models/finalized_model_2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = \"\"\"The Simons Foundation is a private foundation established in 1994 in New York City by Jim and Marilyn Simons. With an annual grants and programs budget of $300 million, the foundationâ€™s mission is to advance the frontiers of research in mathematics and the basic sciences.\n",
    "\n",
    "The foundation pursues its mission through its grant-making division, comprising programs in Mathematics & Physical Sciences, Life Sciences, Education & Outreach and autism research, and through its internal research division, the Flatiron Institute.\n",
    "POSITION SUMMARY\n",
    "\n",
    "Spectrum â€™s award-winning news team is looking for a data analysis intern for the summer.\n",
    "\n",
    "Spectrum is a web-based autism news site intended for scientists, although we are also read by\n",
    "\n",
    "many non-scientists.\n",
    "\n",
    "This is an excellent opportunity for someone comfortable handling large amounts of data and interested in data analysis to gain experience in a fast-paced newsroom.\n",
    "\n",
    "The internship is 18 hours per week, located at our office in Manhattan. This temporary internship is expected to last for 12 weeks. We offer hands-on mentoring in a supportive and fun work environment.\n",
    "\n",
    "POTENTIAL FUNCTIONS/RESPONSIBILITIES\n",
    "\n",
    "Sort, filter and analyze a diverse array of datasets\n",
    "Clean and prepare datasets for analysis\n",
    "Fact-check and document data analyses to ensure that they're accurate and reproducible\n",
    "Create clear, engaging data visualizations using chart-making tools\n",
    "Brainstorm data visualizations to complement news stories and features\n",
    "Identify useful data for future stories\n",
    "Generate story ideas from publicly available data\n",
    "Research, report and write a data-driven story for your portfolio\n",
    "Perform any other duties or tasks as assigned or required.\n",
    "MINIMUM QUALIFICATIONS\n",
    "\n",
    "Education\n",
    "\n",
    "A degree or coursework in journalism or statistics is desired but not required\n",
    "Experience and other requirements\n",
    "\n",
    "Experience filtering, sorting, and aggregating data\n",
    "Strong attention to detail\n",
    "Comfortable with learning new technologies or software\n",
    "Proficient in Microsoft Excel and/or Google Sheets\n",
    "Knowledge of R, Python, HTML/CSS, JavaScript, and/or SQL are helpful\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model from disk\n",
    "\n",
    "tfidf_vectorizer = pickle.load(open('./models/fitted_vectorizer.pickle','rb'))\n",
    "result = clf.predict(tfidf_vectorizer.transform([job]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['0 - 50K', '50K - 70K', '70K - 90K', '90K - 120K', '120K - 150K', '150K - 300K', '300K - 600K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50K - 70K'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[result[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = \"\"\"Role/Responsibilities\n",
    "\n",
    "The Moodyâ€™s Data Science Development Program is a 2 year development opportunity that affords recent graduates the opportunity to work at the intersection of finance and technology; gaining in-depth domain knowledge, applying their skills, and developing new techniques to help solve real-world industry problems. Participants will gain exposure to the different businessâ€™ at Moodyâ€™s and play a vital role in modeling, visualizing, and interpreting critical data to help us continue to use data and technology, together, to transform and create new standards for our industry. Youâ€™ll work with accomplished individuals, who share advanced degrees, certifications, and professional designations in fields such as data science, machine learning, AI, predictive analytics, mathematics, computer science, economics, and engineering.\n",
    "How the program works:\n",
    "The program consists of 2, 12-month assignments, which may be rotated throughout our San Francisco and/or New York, offices.\n",
    "Participants will have the opportunity to interact with senior Data Science and Technology leaders and participate in various key Data Science initiatives. Assignments span across all of the Moodyâ€™s lines of business, providing program participants with a broad set of experiences and opportunities within our firm. Participants will engage in collaborative learning, have guided support from mentors, and be part of our cross-functional professional and social network; building a solid foundation to become future leaders of the company.\n",
    "\n",
    "This full-time position as a Moodyâ€™s data scientist provides a competitive salary, long-term incentive awards, and on-the-job training with real-world projects in a variety of disciplines. To determine assignments, participants will work with a dedicated Program Director who will discuss individual preferences and interests. These preferences are taken into account when assignments are determined, along with the needs of our business.\n",
    "\n",
    "Rotational assignments vary in terms of the technology skills required, and may include assignments in: Analytical Tools and Solutions, Emerging Technologies, Environmental, Social, and Governance (ESG) & Climate Risk, Business Intelligence, Enterprise Applications Systems, Machine Learning, Artificial Intelligence, NLP, Data Science, and Data Engineering.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = clf.predict(tfidf_vectorizer.transform([job]))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'120K - 150K'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[result[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(r\"./models/fitted_vectorizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)  \n",
    "\n",
    "\n",
    "    # model = SGDClassifier(alpha=0.0001, max_iter=500, n_jobs=3).fit(tfidf_vectorizer_vectors, y)\n",
    "\n",
    "    # Create a new pickle file based on the best model\n",
    "    with open(r\"./models/finalized_sgd_model.pkl\", \"wb\") as f:  \n",
    "        pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SGD = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.5, min_df=0.01, ngram_range=(1,3), stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(alpha=0.0001, max_iter=500, n_jobs=3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(max_df=0.5, min_df=0.01, ngram_range=(1, 3),\n",
       "                                 stop_words='english')),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf', SGDClassifier(max_iter=500, n_jobs=3))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['combined_text']\n",
    "y = df['salary_bins'].astype('category')\n",
    "\n",
    "# vectorizer = TfidfVectorizer(sublinear_tf = True, min_df=0.01, max_df=0.5, ngram_range=(1,3), stop_words='english')\n",
    "# X_vect = vectorizer.fit_transform(X)  \n",
    "\n",
    "tdidf = TfidfVectorizer(sublinear_tf = True, min_df=0.01, max_df=0.5, ngram_range=(1,3), stop_words='english')\n",
    "fitted_vectorizer = tdidf.fit(X)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X)\n",
    "\n",
    "\n",
    "with open(r\"./models/fitted_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf_vectorizer_vectors, f)  \n",
    "\n",
    "\n",
    "model = SGDClassifier(alpha=0.0001, max_iter=500, n_jobs=3).fit(tfidf_vectorizer_vectors, y)\n",
    "\n",
    "# Create a new pickle file based on the best model\n",
    "with open(r\"./models/finalized_sgd_model.pkl\", \"wb\") as f:  \n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
